# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MTMKmBvN-86ptK-Ev9zJYu-9EnYpGP20
"""

from flask import Flask, render_template, request
from transformers import AutoModelForCausalLM, AutoTokenizer

app = Flask(__name__)

# Specify the path to your PEFT model on Hugging Face Model Hub
PEFT_MODEL = "heliosbrahma/falcon-7b-sharded-bf16-finetuned-mental-health-conversational"

# Load PEFT model
peft_model = AutoModelForCausalLM.from_pretrained(PEFT_MODEL)

# Load PEFT tokenizer
peft_tokenizer = AutoTokenizer.from_pretrained(PEFT_MODEL)
peft_tokenizer.pad_token = peft_tokenizer.eos_token

class ChatbotModel:
    def __init__(self, peft_model, peft_tokenizer):
        self.peft_model = peft_model
        self.peft_tokenizer = peft_tokenizer

    def generate_response(self, user_input):
        # Method to generate a response using the PEFT model
        peft_encoding = self.peft_tokenizer(user_input, return_tensors="pt")
        peft_outputs = self.peft_model.generate(
            input_ids=peft_encoding.input_ids,
            max_length=256,
            temperature=0.4,
            top_p=0.6,
            repetition_penalty=1.3,
            num_return_sequences=1,
            pad_token_id=self.peft_tokenizer.eos_token_id,
            eos_token_id=self.peft_tokenizer.eos_token_id,
            attention_mask=peft_encoding.attention_mask,
        )
        peft_text_output = self.peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)

        return peft_text_output

# Create an instance of the ChatbotModel with the PEFT model and tokenizer
chatbot = ChatbotModel(peft_model, peft_tokenizer)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/get")
def get_bot_response_route():
    user_text = request.args.get('msg')
    response = chatbot.generate_response(user_text)
    return response

if __name__ == "__main__":
    app.run(debug=True, port=8000)